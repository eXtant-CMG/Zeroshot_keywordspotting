{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sources\n",
        "\n",
        "https://github.com/openai/CLIP/blob/fcab8b6eb92af684e7ff0a904464be7b99b49b88/notebooks/Prompt_Engineering_for_ImageNet.ipynb\n",
        "https://github.com/openai/CLIP#zero-shot-prediction"
      ],
      "metadata": {
        "id": "Zbv-IEDS87vh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnmeAoKFT3d2"
      },
      "source": [
        "# Install the clip package and its dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mmalaUZVfBp2",
        "outputId": "7b9c49c7-692d-47c9-c124-7071c7adaf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7l_mt5_s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7l_mt5_s\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# remove # to download packages\n",
        "\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HKJpNEPRfBp3"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from pkg_resources import packaging\n",
        "from torch.utils.data import Subset\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1NK7pALfBp3",
        "outputId": "0c2d9683-b306-462b-fda6-5bd2b624b78c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 891M/891M [00:11<00:00, 79.6MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# available CLIP models\n",
        "# In this experiment, ViT-L/14@336px is used, as this is also the default model in the original CLIP paper\n",
        "print(clip.available_models())\n",
        "\n",
        "# Loading in model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-L/14@336px', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoVpTWk3fBp4",
        "outputId": "4e05d4db-7fee-469d-f895-ac2dcec10abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 427,944,193\n",
            "Input resolution: 336\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "# Parameters model\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FityzdDCBJjF"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mvrMRC0jBJjF"
      },
      "outputs": [],
      "source": [
        "# Import MNIST dataset\n",
        "\n",
        "from torchvision.datasets import  MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NhWvYQN5BJjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef78bfa-4ac2-4f98-9236-950e72ece511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.cache/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 99640775.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/train-images-idx3-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 38273520.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.cache/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26968427.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3233287.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Loading in MNIST train set\n",
        "# Make sure train is set to true\n",
        "# Train set contains 60.000 images\n",
        "mnist_train = MNIST(root=os.path.expanduser(\"~/.cache\"), download=True, train=True)\n",
        "\n",
        "# Loading in MNIST test set\n",
        "# Test set contains 10.000 images\n",
        "mnist_test = MNIST(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GLvmz7f_BJjG",
        "outputId": "a7907405-35b5-45c4-b189-74a02ade0be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /root/.cache\n",
            "    Split: Train\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: /root/.cache\n",
            "    Split: Test\n"
          ]
        }
      ],
      "source": [
        "# Inspecting number of data points in each set\n",
        "# Check whether all train and test images are loaded in\n",
        "\n",
        "print(mnist_train)\n",
        "print(mnist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yT5VnBQEBJjG"
      },
      "outputs": [],
      "source": [
        "# Takes long to run code on 60.000 images\n",
        "# To test run code on smaller batch, subset of training data is created, containing 10% of the original dataset\n",
        "# Subset of training dataset (10%, or 6000 0f the 60.000 datsets )\n",
        "\n",
        "subset_mnist_train = Subset(mnist_train, indices=range(len(mnist_train) // 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating image and word embeddings"
      ],
      "metadata": {
        "id": "IGQs0Rlk5INO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining classes\n",
        "mnist_classes = ['0','1','2','3','4','5','6','7','8','9',]\n",
        "class_map = {'mnist': mnist_classes}\n",
        "\n",
        "# Defining prompts\n",
        "# Experiment with different prompts, as the entire prompt is encoded\n",
        "# and will influence how similar the image and text embedding are\n",
        "mnist_templates = ['a photo of the number: \"{}\".',]\n",
        "template_map = {'mnist': mnist_templates}"
      ],
      "metadata": {
        "id": "ozYuoSUP67hI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "    return res"
      ],
      "metadata": {
        "id": "neHKTSG78L9M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create text embeddings\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_text_features(dataset_name):\n",
        "    class_names = class_map[dataset_name]\n",
        "    templates = template_map[dataset_name]\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    zeroshot_weights = []\n",
        "    for classname in class_names:\n",
        "        texts = [template.format(classname) for template in templates]\n",
        "        texts = clip.tokenize(texts).to(device)\n",
        "        class_embeddings = model.encode_text(texts)\n",
        "        # embedding size = 768,  (= dimensionality of the encoder layers)\n",
        "        # each template + class gets vector of length 768\n",
        "        # size of tensor is (x, 768), x refers to the number of templates included\n",
        "        # x = rows, 768 = columns\n",
        "        class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # normalized over rows (Frobenius Norm)\n",
        "        # dimension remains (x, 768)\n",
        "        class_embedding = class_embeddings.mean(dim=0)\n",
        "        # Column wise mean is calculated\n",
        "        # Vector of length 768 remains\n",
        "        class_embedding /= class_embedding.norm()\n",
        "        # vector is normalized (Frobenius normalization)\n",
        "        zeroshot_weights.append(class_embedding)\n",
        "        # every input (class + every possible template => mean) is represented as embedding of length 768\n",
        "        # In this case, list of 10 embeddings with length 768\n",
        "    zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)\n",
        "    # list of embeddings is concatenated in matrix\n",
        "    # embeddings are joined on column level\n",
        "\n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    return zeroshot_weights"
      ],
      "metadata": {
        "id": "NqGHtCkBhVuu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = []\n",
        "image_labels = []\n",
        "# !! Loading in the data: images + ground truth label (HAVE TO BE LOADED IN IN THE SAME ORDER)\n",
        "for image, class_id in subset_mnist_train:\n",
        "    # Preprocess: pixel intensity is normalized using dataset mean and standard deviation.\n",
        "    # Input images are resized and center-cropped to conform with the expected image resolution.\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    # unsqueeze(x), returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    # x refers to the index at which to insert the singleton dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_feature = model.encode_image(image_input)\n",
        "        # matrix with dimenension (1, 768)\n",
        "\n",
        "    image_feature /= image_feature.norm()\n",
        "    # image features are normalized, if dim is None, the norm is calculated across all dimensions of input\n",
        "\n",
        "    image_features.append(image_feature)\n",
        "    # list of embedding (length 768) for each image\n",
        "    image_labels.append(class_id)\n",
        "    # Class id (the factorized ground truth) is added in a list of tensors"
      ],
      "metadata": {
        "id": "kZuojtfziDLB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = torch.stack(image_features, dim=-1).to(device)\n",
        "# separate embeddings are concatenated on the second of 2 dimensions, (so on rows)\n",
        "# The final matrix has the shape [768, 10]\n",
        "\n",
        "# Squeeze to get a tensor returned with all the dimensions of input of size 1 removed\n",
        "image_features = image_features.squeeze()"
      ],
      "metadata": {
        "id": "L12n50b9iXvU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text feature\n",
        "text_features = extract_text_features('mnist')"
      ],
      "metadata": {
        "id": "wWFeEaNwimoU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute top-1 accuracy\n",
        "logits = (100. * image_features.T @ text_features).softmax(dim=-1)\n",
        "# image features are multiplied by text features, softmax converts real numbers into probability distribution\n",
        "# Output tensor lies in range 0-1 and sums to 1\n",
        "image_labels = torch.tensor(image_labels).unsqueeze(dim=1).to(device)\n",
        "\n",
        "top1_acc = accuracy(logits, image_labels, (1,))\n",
        "\n",
        "print(f'top-1 accuracy for = dataset: {top1_acc[0]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAlxfjWJkcCM",
        "outputId": "4e26a062-b889-4980-fc9d-ca9641798242"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top-1 accuracy for = dataset: 81.400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing highest scoring image"
      ],
      "metadata": {
        "id": "dKsKlD-TPxXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_number = input(\"which number are you looking for? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9gAZs2JP8Kj",
        "outputId": "9faa9352-9aef-4eb3-9e53-e861164adb3a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "which number are you looking for? 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the text prompt for the target number\n",
        "target_text = template_map['mnist'][0].format(target_number)\n",
        "target_text = clip.tokenize([target_text]).to(device)\n",
        "\n",
        "# Encode the text prompt to get its feature\n",
        "target_text_feature = model.encode_text(target_text)\n",
        "target_text_feature /= target_text_feature.norm()\n",
        "\n",
        "# Transpose the image features for compatibility\n",
        "transposed_image_features = image_features.T\n",
        "\n",
        "# Calculate similarity scores for each image\n",
        "similarity_scores = transposed_image_features @ target_text_feature.T\n",
        "\n",
        "# Find the index of the image with the highest similarity score\n",
        "best_image_index = torch.argmax(similarity_scores)\n",
        "\n",
        "# Retrieve the corresponding image\n",
        "best_image = subset_mnist_train[best_image_index][0]\n",
        "\n",
        "# Print the class ID of the best image\n",
        "best_image_class_id = subset_mnist_train[best_image_index][1]\n",
        "print(\"Best Image Class ID:\", best_image_class_id)\n",
        "\n",
        "# Convert the PIL Image to a NumPy array\n",
        "best_image_np = np.array(best_image)\n",
        "\n",
        "# Display the best image\n",
        "plt.imshow(best_image_np, cmap='gray')  # Assuming it's a grayscale image\n",
        "plt.title(f\"Image with highest similarity to {target_number}\")\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "83UbasbOx1-b",
        "outputId": "670436ed-53c3-4740-bd6e-5ba077e4d45f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Image Class ID: 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnqGEWXMQSDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FityzdDCBJjF"
      ],
      "provenance": [],
      "gpuType": "V100",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}