{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sources\n",
        "\n",
        "https://github.com/openai/CLIP/blob/fcab8b6eb92af684e7ff0a904464be7b99b49b88/notebooks/Prompt_Engineering_for_ImageNet.ipynb\n",
        "https://github.com/openai/CLIP#zero-shot-prediction"
      ],
      "metadata": {
        "id": "Zbv-IEDS87vh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnmeAoKFT3d2"
      },
      "source": [
        "# Install the clip package and its dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "collapsed": true,
        "id": "mmalaUZVfBp2",
        "outputId": "8ecc12b0-64c5-4573-c0e4-ff54fc8cedfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9hxn110e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-9hxn110e\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0)\n",
            "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369500 sha256=e973bef6e37a4fd8bbaea5218355021195bf909318c7e8581c02411551bf834a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t3tarvz3/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: wcwidth, ftfy, clip\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.10\n",
            "    Uninstalling wcwidth-0.2.10:\n",
            "      Successfully uninstalled wcwidth-0.2.10\n",
            "Successfully installed clip-1.0 ftfy-6.1.3 wcwidth-0.2.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# remove # to download packages\n",
        "\n",
        "#! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKJpNEPRfBp3"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from pkg_resources import packaging\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1NK7pALfBp3",
        "outputId": "2b4d22ed-cdf8-4707-f02b-6ccd4fb3b0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 891M/891M [00:11<00:00, 83.4MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# available CLIP models\n",
        "# In this experiment, ViT-L/14@336px is used, as this is also the default model in the original CLIP paper\n",
        "print(clip.available_models())\n",
        "\n",
        "# Loading in model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-L/14@336px', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoVpTWk3fBp4",
        "outputId": "2f63c2af-d652-4a6c-d0f8-a89a61746b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 427,944,193\n",
            "Input resolution: 336\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "# Parameters model\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FityzdDCBJjF"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvrMRC0jBJjF"
      },
      "outputs": [],
      "source": [
        "# Import MNIST dataset\n",
        "\n",
        "from torchvision.datasets import  MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhWvYQN5BJjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f5e174-b35c-43c6-dc54-0645827c3e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.cache/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 99372847.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/train-images-idx3-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 24711483.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.cache/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26840631.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3562178.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.cache/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Loading in MNIST train set\n",
        "# Make sure train is set to true\n",
        "# Train set contains 60.000 images\n",
        "mnist_train = MNIST(root=os.path.expanduser(\"~/.cache\"), download=True, train=True)\n",
        "\n",
        "# Loading in MNIST test set\n",
        "# Test set contains 10.000 images\n",
        "mnist_test = MNIST(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLvmz7f_BJjG",
        "outputId": "843e4448-7fd8-420d-bdf7-f220ca3bc8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /root/.cache\n",
            "    Split: Train\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: /root/.cache\n",
            "    Split: Test\n"
          ]
        }
      ],
      "source": [
        "# Inspecting number of data points in each set\n",
        "# Check whether all train and test images are loaded in\n",
        "\n",
        "print(mnist_train)\n",
        "print(mnist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT5VnBQEBJjG"
      },
      "outputs": [],
      "source": [
        "# Takes long to run code on 60.000 images\n",
        "# To test run code on smaller batch, subset of training data is created, containing 10% of the original dataset\n",
        "# Subset of training dataset (10%, or 6000 0f the 60.000 datsets )\n",
        "\n",
        "subset_mnist_train = Subset(mnist_train, indices=range(len(mnist_train) // 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating image and word embeddings"
      ],
      "metadata": {
        "id": "IGQs0Rlk5INO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining classes\n",
        "mnist_classes = ['0','1','2','3','4','5','6','7','8','9',]\n",
        "class_map = {'mnist': mnist_classes}\n",
        "\n",
        "# Defining prompts\n",
        "# Experiment with different prompts, as the entire prompt is encoded\n",
        "# and will influence how similar the image and text embedding are\n",
        "mnist_templates = ['a photo of the number: \"{}\".',]\n",
        "template_map = {'mnist': mnist_templates}"
      ],
      "metadata": {
        "id": "ozYuoSUP67hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "    return res"
      ],
      "metadata": {
        "id": "neHKTSG78L9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create text embeddings\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_text_features(dataset_name):\n",
        "    class_names = class_map[dataset_name]\n",
        "    templates = template_map[dataset_name]\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    zeroshot_weights = []\n",
        "    for classname in class_names:\n",
        "        texts = [template.format(classname) for template in templates]\n",
        "        texts = clip.tokenize(texts).to(device)\n",
        "        class_embeddings = model.encode_text(texts)\n",
        "        # embedding size = 768,  (= dimensionality of the encoder layers)\n",
        "        # each template + class gets vector of length 768\n",
        "        # size of tensor is (x, 768), x refers to the number of templates included\n",
        "        # x = rows, 768 = columns\n",
        "        class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # normalized over rows (Frobenius Norm)\n",
        "        # dimension remains (x, 768)\n",
        "        class_embedding = class_embeddings.mean(dim=0)\n",
        "        # Column wise mean is calculated\n",
        "        # Vector of length 768 remains\n",
        "        class_embedding /= class_embedding.norm()\n",
        "        # vector is normalized (Frobenius normalization)\n",
        "        zeroshot_weights.append(class_embedding)\n",
        "        # every input (class + every possible template => mean) is represented as embedding of length 768\n",
        "        # In this case, list of 10 embeddings with length 768\n",
        "    zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)\n",
        "    # list of embeddings is concatenated in matrix\n",
        "    # embeddings are joined on column level\n",
        "\n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    return zeroshot_weights"
      ],
      "metadata": {
        "id": "NqGHtCkBhVuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = []\n",
        "image_labels = []\n",
        "# !! Loading in the data: images + ground truth label (HAVE TO BE LOADED IN IN THE SAME ORDER)\n",
        "for image, class_id in subset_mnist_train:\n",
        "    # Preprocess: pixel intensity is normalized using dataset mean and standard deviation.\n",
        "    # Input images are resized and center-cropped to conform with the expected image resolution.\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "    # unsqueeze(x), returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    # x refers to the index at which to insert the singleton dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_feature = model.encode_image(image_input)\n",
        "        # matrix with dimenension (1, 768)\n",
        "\n",
        "    image_feature /= image_feature.norm()\n",
        "    # image features are normalized, if dim is None, the norm is calculated across all dimensions of input\n",
        "\n",
        "    image_features.append(image_feature)\n",
        "    # list of embedding (length 768) for each image\n",
        "    image_labels.append(class_id)\n",
        "    # Class id (the factorized ground truth) is added in a list of tensors"
      ],
      "metadata": {
        "id": "kZuojtfziDLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = torch.stack(image_features, dim=-1).to(device)\n",
        "# separate embeddings are concatenated on the second of 2 dimensions, (so on rows)\n",
        "# The final matrix has the shape [768, 10]\n",
        "\n",
        "# Squeeze to get a tensor returned with all the dimensions of input of size 1 removed\n",
        "image_features = image_features.squeeze()"
      ],
      "metadata": {
        "id": "L12n50b9iXvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract text feature\n",
        "text_features = extract_text_features('mnist')"
      ],
      "metadata": {
        "id": "wWFeEaNwimoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute top-1 accuracy\n",
        "logits = (100. * image_features.T @ text_features).softmax(dim=-1)\n",
        "# image features are multiplied by text features, softmax converts real numbers into probability distribution\n",
        "# Output tensor lies in range 0-1 and sums to 1\n",
        "image_labels = torch.tensor(image_labels).unsqueeze(dim=1).to(device)\n",
        "\n",
        "top1_acc = accuracy(logits, image_labels, (1,))\n",
        "\n",
        "print(f'top-1 accuracy for = dataset: {top1_acc[0]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAlxfjWJkcCM",
        "outputId": "04485678-1d5e-4b7f-8d91-acdf324ab38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top-1 accuracy for = dataset: 81.400\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FityzdDCBJjF"
      ],
      "provenance": [],
      "gpuType": "V100",
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}